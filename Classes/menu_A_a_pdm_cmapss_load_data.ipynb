{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = 6 color = green><b> Predicitive Maintenance / 智能性维护实例 </b></font>\n",
    "# Menu A-a: Load Load / 读取数据 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries / 工具库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np \n",
    "from pathlib import Path \n",
    "import zipfile\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-requisite / 准备工作\n",
    "* Download / 下载压缩数据 from https://onedrive.live.com/?cid=7CAD6DA55D313592&id=7CAD6DA55D313592%21159&parId=7CAD6DA55D313592%21158&o=OneUp \n",
    "* Save CMAPSS zipfile to  C:/pdm/zipraw / 把下载的压缩文件存放在 C:/pdm/zipraw    \n",
    "  （ or / 或 d:/pdm/zipraw）\n",
    "* Unzipped data will be stored in raw data folder \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare folders / 准备文件夹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_paths(data_parent_folder = None):\n",
    "    file_paths = {}\n",
    "    if not data_parent_folder:\n",
    "        data_parent_folder = os.path.dirname(os.getcwd())\n",
    "    file_paths[\"parent_folder\"] = data_parent_folder\n",
    "    file_paths[\"raw_data_path\"] = data_parent_folder + '/raw_data'\n",
    "    file_paths[\"zip_data_path\"] = data_parent_folder + '/zipraw'\n",
    "    file_paths[\"unzip_to_path\"] = data_parent_folder + '/raw_data'\n",
    "    return file_paths "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### execute for this notebook/执行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'parent_folder': 'c:\\\\classes\\\\pdm', 'raw_data_path': 'c:\\\\classes\\\\pdm/raw_data', 'zip_data_path': 'c:\\\\classes\\\\pdm/zipraw', 'unzip_to_path': 'c:\\\\classes\\\\pdm/raw_data'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "FILE_PATHS = get_file_paths()\n",
    "print(FILE_PATHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unzip Ulitity / 解压\n",
    "* Use zipfile library to unzip / 用 zipfile 工具包解压"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unzip_files(zip_file_name = None,  remove_zipped = False):\n",
    "    if not zip_file_name:  \n",
    "        zip_file_name = f'{FILE_PATHS[\"zip_data_path\"]}/CMAPSS.zip' \n",
    "\n",
    "    if not os.path.exists(FILE_PATHS[\"zip_data_path\"]):\n",
    "        os.makedirs(FILE_PATHS[\"zip_data_path\"])\n",
    "\n",
    "    with zipfile.ZipFile(zip_file_name, 'r') as zip_ref:\n",
    "        zip_ref.extractall(FILE_PATHS[\"unzip_to_path\"])\n",
    "\n",
    "    if remove_zipped: \n",
    "        os.remove(zip_file_name)\n",
    "\n",
    "    return [FILE_PATHS[\"unzip_to_path\"] + \"/\" + file for file in os.listdir(FILE_PATHS[\"unzip_to_path\"])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File System Manipulations / 文件处理\n",
    "* Use regex / 用Regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_data_files(): \n",
    "    return [FILE_PATHS[\"raw_data_path\"] + \"/\" + file for file in os.listdir(FILE_PATHS[\"raw_data_path\"])] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files_regex(file_name_str = \"test\"): \n",
    "    raw_files = list_data_files()\n",
    "    regex = re.compile(f\".+{file_name_str}.+gz\")  \n",
    "    raw_data_files = [f for f in raw_files if re.match(regex, f)]\n",
    "    return raw_data_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load to dataframe / 读取到 pandas DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data_files(file_name_str = \"train\", use_pd = True, sep = \" \", columns = None):  \n",
    "    if not columns:\n",
    "        columns=[\"id\",\"cycle\",\"op1\",\"op2\",\"op3\",\"sensor1\",\"sensor2\",\"sensor3\",\"sensor4\",\"sensor5\",\"sensor6\",\"sensor7\",\"sensor8\",\n",
    "            \"sensor9\",\"sensor10\",\"sensor11\",\"sensor12\",\"sensor13\",\"sensor14\",\"sensor15\",\"sensor16\",\"sensor17\",\"sensor18\",\"sensor19\"\n",
    "            ,\"sensor20\",\"sensor21\" ]  \n",
    "\n",
    "    raw_data_files = get_files_regex(file_name_str =file_name_str) \n",
    "\n",
    "    df_total =  pd.DataFrame() \n",
    "    for f in raw_data_files: \n",
    "        if use_pd:\n",
    "            df_ = pd.read_csv(f, compression='gzip',index_col = False, names = columns, sep=' ')\n",
    "        else: \n",
    "            df_= pd.DataFrame(np.loadtxt(f), columns=columns) \n",
    "        df_[[\"id\", \"cycle\"]] = df_[[\"id\", \"cycle\"]].astype(int)\n",
    "            \n",
    "        flag = re.findall(r\"FD\\d{3}\", str(f))[0]\n",
    "        df_[\"Flag\"] = flag \n",
    "        if df_total.empty:\n",
    "            df_total = df_.copy()\n",
    "        else: \n",
    "            df_total = pd.concat([df_total, df_], axis = 0 ) \n",
    "    \n",
    "    return df_total \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_result(file_name_str = \"RUL_FD\", use_pd = True, sep = \" \", columns = None):\n",
    "    raw_data_files = get_files_regex(file_name_str =file_name_str)  \n",
    "    if not columns:\n",
    "        columns = [\"rul\"]\n",
    "\n",
    "    df_result =  pd.DataFrame() \n",
    "    for f in raw_data_files:\n",
    "        if use_pd: \n",
    "            df_ = pd.read_csv(f, compression='gzip', index_col = False, names = columns, sep = sep)\n",
    "        else:\n",
    "            df_= pd.DataFrame(np.loadtxt(f), columns = columns) \n",
    "        flag = re.findall(r\"FD\\d{3}\", str(f))[0]\n",
    "        df_[\"Flag\"] = flag \n",
    "        if df_result.empty:\n",
    "            df_result = df_.copy()\n",
    "        else: \n",
    "            df_result = pd.concat([df_result, df_], axis = 0 ) \n",
    "    return df_result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sum Up: prepare train, test and result file / 主程序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dfs(use_pd = True, sep = \" \"): \n",
    "      \n",
    "     columns=[\"id\",\"cycle\",\"op1\",\"op2\",\"op3\",\"sensor1\",\"sensor2\",\"sensor3\",\"sensor4\",\"sensor5\",\"sensor6\",\"sensor7\",\"sensor8\",\n",
    "          \"sensor9\",\"sensor10\",\"sensor11\",\"sensor12\",\"sensor13\",\"sensor14\",\"sensor15\",\"sensor16\",\"sensor17\",\"sensor18\",\"sensor19\"\n",
    "          ,\"sensor20\",\"sensor21\" ] \n",
    "\n",
    "     # Train\n",
    "     df_train = read_data_files( file_name_str = \"train\", use_pd = use_pd, sep = \" \", columns = columns)\n",
    "     # Test\n",
    "     df_test = read_data_files( file_name_str = \"test\", use_pd = use_pd, sep = \" \", columns = columns)\n",
    "\n",
    "     resul_columns = [\"rul\"]\n",
    "     df_result = read_result(file_name_str = \"RUL_FD\", \\\n",
    "          use_pd = use_pd, sep =sep, columns = resul_columns)\n",
    " \n",
    "     df_train.iloc[:, [0,1]] = df_train.iloc[:, [0,1]].astype(int)\n",
    "     df_test.iloc[:, [0,1]] = df_test.iloc[:, [0,1]].astype(int) \n",
    "\n",
    "     df_max = df_test.groupby([\"Flag\",\"id\"])[\"cycle\"].max().reset_index()\n",
    "     df_result = df_result.reset_index()\n",
    "     df_result[\"id\"] = df_result.groupby(\"Flag\")[\"index\"].rank(\"first\", ascending = True).astype(int)\n",
    "     df_result.drop(columns = [\"index\"], inplace = True)\n",
    "     \n",
    "     df_result = df_result.merge(df_max, on = [\"Flag\", \"id\"], how = \"inner\")\n",
    "      \n",
    "     df_result[\"rul_failed\"] = df_result[\"rul\"] + df_result[\"cycle\"]\n",
    "\n",
    "     df_test = df_test.merge(df_result[[\"rul_failed\", \"Flag\", \"id\"]], on = [\"Flag\", \"id\"], how = \"inner\")\n",
    "     df_test[\"remaining_rul\"] = df_test[\"rul_failed\"] - df_test[\"cycle\"]\n",
    "\n",
    "     #df_test[[\"rul_failed\", \"remaining_rul\"]] = df_test[[\"rul_failed\", \"remaining_rul\"]].astype(int)\n",
    "     return df_train, df_test, df_result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Call  prepare_dfs（）  / 调用主程序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test, df_result = prepare_dfs(use_pd=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "3361607028a07fbbc80f378608ecaa72f5ffca90043f5aaa255f4292887cf27e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
